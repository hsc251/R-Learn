---
title: "Data Science Capstone Milestone"
author: "Hsin Chih Chen"
date: "1/17/2022"
output: html_document
---

## Abstract

This milestone report is for Johns Hopkins University's data Science Capstone, and the scope of the capstone is to use existing text corpus to train the predictive text model. 

The general setup will be applied throughout following section description with attached r codes.

## Preliminary Setup

This section contains the R code for libraries and parallel computing setups.

```{r Library Loading, message = FALSE, warning = FALSE}
library(doParallel)
library(tm)
library(stringr)
library(RWeka)
library(kableExtra)
library(knitr)
library(DT)
library(qdap)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(tm)
library(ngram)
library(data.table)
library(RColorBrewer)
library(stringi)
```

```{r Parallel Computing Setup, message = FALSE, warning = FALSE}
set.seed(369)
core_num <- detectCores() - 2
registerDoParallel(core_num, cores = core_num)
```


## Data Loading 

This section will setup the connection from the raw files, the English version of the corpus will be applied for analysis.

```{R Data Import, warning = FALSE}
# The file path will be depending on where the file is loaded.

# Blog Lines Import
con_blog <- file("C:/Users/Colin Chen/Documents/final/en_US/en_US.blogs.txt", "rb")
eng_blog <- read_lines(con_blog)
close.connection(con_blog)
rm(con_blog)

# News Lines Import
con_news <- file("C:/Users/Colin Chen/Documents/final/en_US/en_US.news.txt", "rb")
eng_news <- read_lines(con_news)
close.connection(con_news)
rm(con_news)

# Twitter Lines Import
con_twitter <- file("C:/Users/Colin Chen/Documents/final/en_US/en_US.twitter.txt", "rb")
eng_twitter <- read_lines(con_twitter)
close.connection(con_twitter)
rm(con_twitter)
```

After the word of lines are imported, summarizing the respective media files.

```{R Text File Summaries, warning = FALSE}
# Identify the size for each file
mega = 2^20

size_blog <- round(file.info("C:/Users/Colin Chen/Documents/final/en_US/en_US.blogs.txt")$size/mega, 2)
size_news <- round(file.info("C:/Users/Colin Chen/Documents/final/en_US/en_US.news.txt")$size/mega, 2)
size_twitter <- round(file.info("C:/Users/Colin Chen/Documents/final/en_US/en_US.twitter.txt")$size/mega, 2)

# Identify number of lines in each media
lines_blog <- length(eng_blog)
lines_news <- length(eng_news)
lines_twitter <- length(eng_twitter)

# Identify number of characters in each media
char_blog <- round(mean(nchar(eng_blog)),2)
char_news <- round(mean(nchar(eng_news)),2)
char_twitter <- round(mean(nchar(eng_twitter)),2)

# Identify number of words in each media
word_blog <- wordcount(eng_blog, sep = " ")
word_news <- wordcount(eng_news, sep = " ")
word_twitter <- wordcount(eng_twitter, sep = " ")

# Identify word per line (WPL) in each media
wpl_blog <- round(word_blog/lines_blog,2)
wpl_news <- round(word_news/lines_news,2)
wpl_twitter <- round(word_twitter/lines_twitter,2)

# Summarize the data frame
file_names <- c("EN Blogs", "EN News", "EN Twitter")
file_sizes <- c(size_blog, size_news, size_twitter)
file_char <- c(char_blog, char_news,char_twitter)
file_lines <- c(lines_blog, lines_news, lines_twitter)
file_words <- c(word_blog, word_news, word_twitter)
avg_wpl <- c(wpl_blog, wpl_news, wpl_twitter)

file_summary <- data.frame(file_names, file_sizes, file_lines, file_char, file_words, avg_wpl)

colnames(file_summary) <- c("Media", "Size (MB)", "# Lines", "Avg Chars", "# Words", "Avg WPL")

# Make it as a kable
kable(file_summary) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Sample Fraction for Media source

After summarizing the file's raw content, sampling fraction of the word gram shall be considered to analyze the words.

```{R Word Sampling, warning = FALSE}
# Setup sampling rate & random seeding
sample_rate = 0.01
set.seed(369)

# Sample the number based on the total fraction of media
ss_blog <- lines_blog * sample_rate
ss_news <- lines_news * sample_rate
ss_twitter <- lines_twitter * sample_rate

# Generate sample
sample_blog <- sample(eng_blog, ss_blog, replace = FALSE)
sample_news <- sample(eng_news, ss_news, replace = FALSE)
sample_twitter <- sample(eng_twitter, ss_twitter, replace = FALSE)

# Remove non-English characters from sampled data
sample_blog <- iconv(sample_blog, "latin1","ASCII", sub = "")
sample_news <- iconv(sample_news, "latin1","ASCII", sub = "")
sample_twitter <- iconv(sample_twitter, "latin1","ASCII", sub = "")

# Combine all data set as one
sample_sum <- c(sample_blog, sample_news, sample_twitter)

# Sampled Text Saving and examine the dimension
writeLines(sample_sum, "C:/Users/Colin Chen/Documents/final/en_US/en_US.sample.txt")
saveRDS(sample_sum,"C:/Users/Colin Chen/Documents/final/en_US/en_US.sample.rds")

# Lines from sample_sum
ss_lines <- length(sample_sum)

# number of words from sample_sum
ss_words <- sum(stri_count_words(sample_sum))

# Remove variables which are no longer required
rm(eng_blog, eng_news, eng_twitter, sample_blog, sample_news, sample_twitter)
```

But before the n-gram calculation, samples shall be cleaned without ambiguous information. The following code will execute it.

```{R Corpus Demonstration, warning = FALSE}
# Generate text corpus and show example
text_corpus <- Corpus(VectorSource(sample_sum)) 

print(as.character(text_corpus[[1]]))
```

The next step is to use the sampled data to filter and tidy up the text file (a.k.a corpus) for sampling. In this case, the customized function named `corpus_clean` will be generated to execute the following command:

  1. Remove, URL, Twitter handles and email patterns while convert to spaces by custom content transformer
  2. Convert all words to lowercases
  3. Remove common English stop words.
  4. Remove punctuation marks
  5. Remove numeric values
  6. Trim whitespace.
  7. Remove profanity
  8. Convert to a plain text document
  
```{R Corpus Setup, warning = FALSE}
# Read profanity language for input
profanity <- read.csv("C:/Users/Colin Chen/Documents/final/en_US/en_profanity.csv", header = FALSE, sep = "\n") %>%
  iconv("latin1", "ASCII", sub = "") 

# Establish corpus function to have tidy corpus
# The function

space_conv <- function(x, pattern){
  gsub(pattern, " ", x)
} 

corpus_clean <- function (source_file) {
  docu <- VCorpus(VectorSource(source_file)) %>%
    
# Remove URL, Twitter Handles and email pattern
    tm_map(space_conv, "(f|ht)tp(s?)://(.*)[.][a-z]+") %>%
    tm_map(space_conv, "@[^\\s]+") %>%
    tm_map(space_conv, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b") %>%
    tm_map(removeWords, profanity) %>%
    tm_map(tolower) %>%
    tm_map(removeWords, stopwords("english")) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers) %>%
    tm_map(stripWhitespace) %>%
    tm_map(PlainTextDocument)
  
  return(docu)
}

clean_corpus <- corpus_clean(sample_sum)

# Save clean corpus
saveRDS(clean_corpus, file = "C:/Users/Colin Chen/Documents/final/en_US/en_US.corpus.rds")

# Convert corpus to a data frame
clean_text <- data.frame(text = unlist(sapply(clean_corpus, '[', "content")), stringsAsFactors = FALSE)

writeLines(clean_text$text, "C:/Users/Colin Chen/Documents/final/en_US/en_US.corpus.txt")

# Remove unused terms to spare up memories
rm(text_corpus, sample_sum)
```

## Initial Exploratory Data Analysis

Before executing the exploratory analysis with the cleaned corpus, document term matrix shall be generated while remove necessary sparse terms.

```{R DTM Generation, warning = FALSE}
# generate the Document Term Matrix while eliminating sparse terms

tdm_corpus <- TermDocumentMatrix(clean_corpus) %>%
  removeSparseTerms(sparse = 0.995)

# Convert the word counts before summarizing into the data frame
freq <- sort(rowSums(as.matrix(tdm_corpus)), decreasing = TRUE)

wordCount <- data.frame(word = names(freq), count = freq)
```

```{R Word Graph Setup, warning = FALSE}
## Top 12 unigram across the media
top12 <- wordCount[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#25BA95", alpha = 0.8) +
  labs(x = "Unigram", y = "Frequency", title = "Top 12 unigram across the media") +
  theme(axis.text.x = element_text(size = 7),
                       axis.text.y = element_text(size = 7),
                       axis.title = element_text(size = 8.5),
                       title = element_text(size = 9)) +
  geom_text(aes(label = wordCount[1:12,]$count), 
                           size = 3, vjust = -0.2) +
  ylim(0,3800)

# Display actual graph
top12
```

```{R Wordcloud Setup, warning = FALSE}
## Generate Wordcloud
png("jhu_ds_milestone.png", width=12 , height=8, units='in', res=300)
wordcloud(words = wordCount$word, freq = wordCount$count, min.freq = 150, scale = c(7, .25),max.words = 150, rot.per = 0.3, colors = brewer.pal(9, "Set2"))

## Remove unnecessary terms to free up memory
rm(tdm_corpus,freq, top12, wordCount)
```


## n-gram Tokenization & Distribution
After understanding the prelimnary distribution from the cleaned corpus, it's time to setup the ngram to distinguish the existing data.

But before the split for ngram, the preliminary tokenization shall be initiated to split the ngrams

```{r Tokenize Setup, warning = FALSE, message = FALSE}
# Unigram
token_n1 <- function(x){
  NGramTokenizer(x, Weka_control(min = 1, max = 1))
}  
  
# Bigram
token_n2 <- function(x){
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}  

# Trigram
token_n3 <- function(x){
  NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
```

```{R ngram Matrix Setup, message = FALSE, warning = FALSE}
# Generate respective matrices for the ngrams
matrix_n1 <- TermDocumentMatrix(clean_corpus, control = list(tokenize = token_n1))

matrix_n2 <- TermDocumentMatrix(clean_corpus, control = list(tokenize = token_n2))

matrix_n3 <- TermDocumentMatrix(clean_corpus, control = list(tokenize = token_n3))

# Eliminate sparse terms for each n gram and obtain frequency for n grams.
matrix_n1f <- sort(rowSums(as.matrix(removeSparseTerms(matrix_n1, 0.999))), decreasing = TRUE)

matrix_n2f <- sort(rowSums(as.matrix(removeSparseTerms(matrix_n2, 0.999))), decreasing = TRUE)

matrix_n3f <- sort(rowSums(as.matrix(removeSparseTerms(matrix_n3, 0.9999))), decreasing = TRUE)

# Generate data frame for the plotting in n gram
matrix_n1df <- data.frame(word = names(matrix_n1f), count = matrix_n1f)
matrix_n2df <- data.frame(word = names(matrix_n2f), count = matrix_n2f)
matrix_n3df <- data.frame(word = names(matrix_n3f), count = matrix_n3f)
```

```{R ngram Graphing, message = FALSE, warning = FALSE}
# Top 12 unigram across the media
top12_n1 <- matrix_n1df[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#0394fc", alpha = 0.8) +
  labs(x = "Unigram", y = "Frequency", title = "Top 12 unigram across the media") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 8.5),
        title = element_text(size = 9)) +
  geom_text(aes(label = matrix_n1df[1:12,]$count), size = 3, vjust = -0.2) +
  ylim(0,3800)

# Top 12 bigram across the media
top12_n2 <- matrix_n2df[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#5e0599", alpha = 0.8) +
  labs(x = "Bigram", y = "Frequency", title = "Top 12 biigram across the media") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 8.5),
        title = element_text(size = 9)) +
  geom_text(aes(label = matrix_n2df[1:12,]$count), size = 3, vjust = -0.2) +
  ylim(0,310)

# Top 12 trigram across the media
top12_n3 <- matrix_n3df[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#649e97", alpha = 0.8) +
  labs(x = "Trigram", y = "Frequency", title = "Top 12 trigram across the media") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 8.5),
        title = element_text(size = 9)) +
  geom_text(aes(label = matrix_n3df[1:12,]$count), size = 3, vjust = -0.2) +
  ylim(0,35)

# Display Graphs
top12_n1
top12_n2
top12_n3
```

Based on the studying of the graph, the top 5 terms for respective media usage are summarized as follows:
```{R ngram summary, message = FALSE, warning = FALSE}
ngrams <- c("unigram", "bigram","trigram")
no1word <- c(matrix_n1df[1,1],matrix_n2df[1,1],matrix_n3df[1,1])
no2word <- c(matrix_n1df[2,1],matrix_n2df[2,1],matrix_n3df[2,1])
no3word <- c(matrix_n1df[3,1],matrix_n2df[3,1],matrix_n3df[3,1])
no4word <- c(matrix_n1df[4,1],matrix_n2df[4,1],matrix_n3df[4,1])
no5word <- c(matrix_n1df[5,1],matrix_n2df[5,1],matrix_n3df[5,1])

ngram_summary <- data.frame(ngrams, no1word, no2word, no3word, no4word, no5word)

colnames(ngram_summary) <- c("ngrams", "Top 1", "Top 2", "Top 3", "Top 4", "Top 5")

kable(ngram_summary) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Path Forward

After summarizing the existing data from SwiftKey and understand the current distribution between the words. The upcoming challenge is to establish a predictive model which will be deployed as a Shiny App for predicting in textbox.

The predictive algorithm will be developed via ngram models with word frequency similar towards the milestone report.

The strategy can be breaking down to the following 

  1. Find ways to neutralize the processing time for data sets.
  2. Use the text mining algorithm to obtain necessary ngrams.
  3. Review each ngram's methodology for performance.
  4. Decide necessary algorithm or better dictionary for better n-gram splits.
  5. Have different sets of non-common and common words for evaluation.