---
title: "Data Science Capstone Milestone"
author: "Hsin Chih Chen"
date: "1/17/2022"
output: html_document
---

## Abstract

This milestone report is for Johns Hopkins University's data Science Capstone, 
and the scope of the capstone is to use existing text corpus to train the 
predictive text model. 

The general setup (for milestone analysis and training data preparation) will 
be applied throughout following section description with attached r codes.

## Preliminary Setup

This section contains the R code for libraries and parallel computing setups.

```{r Library Loading, message = FALSE, warning = FALSE}
library(doParallel)
library(tm)
library(stringi)
library(stringr)
library(RWeka)
library(kableExtra)
library(knitr)
library(DT)
library(qdap)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(tm)
library(ngram)
library(data.table)
library(RColorBrewer)
```

```{r Parallel Computing Setup, message = FALSE, warning = FALSE}
set.seed(369)
core_num <- detectCores() - 2
registerDoParallel(core_num, cores = core_num)
```


## Data Loading 

This section will setup the connection from the raw files, the English version of the corpus will be applied for analysis.

```{R Environment Setup & Data Download, warning = FALSE}
# Set the working directory to the designated location.
setwd("C:/Users/Colin Chen/Documents/DS_Capstone")

# Download Raw Data
fileURL <- c("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")


if (!file.exists('rd')) {
  dir.create('rd')
}

if (!file.exists("rd/final/en_US")) {
  tempFile <- tempfile()
  download.file(fileURL, tempFile)
  unzip(tempFile, exdir = "rd")
  unlink(tempFile)
  rm(tempFile)
}
```

```{R Data Import, warning = FALSE}
# The file path will be depending on where the file is loaded.
# Remark: rd = raw data directory

# Blog Lines Import
con_blog <- file("rd/final/en_US/en_US.blogs.txt", "rb")
eng_blog <- read_lines(con_blog)
close.connection(con_blog)
rm(con_blog)

# News Lines Import
con_news <- file("rd/final/en_US/en_US.news.txt", "rb")
eng_news <- read_lines(con_news)
close.connection(con_news)
rm(con_news)

# Twitter Lines Import
con_twitter <- file("rd/final/en_US/en_US.twitter.txt", "rb")
eng_twitter <- read_lines(con_twitter)
close.connection(con_twitter)
rm(con_twitter)
```

After the word of lines are imported, summarizing the respective media files.

```{R Text File Summaries, warning = FALSE}
# Identify the size for each file
mega = 2^20

size_blog <- round(file.info("rd/final/en_US/en_US.blogs.txt")$size/mega, 2)
size_news <- round(file.info("rd/final/en_US/en_US.news.txt")$size/mega, 2)
size_twitter <- round(file.info("rd/final/en_US/en_US.twitter.txt")$size/mega, 2)

# Identify number of lines in each media
lines_blog <- length(eng_blog)
lines_news <- length(eng_news)
lines_twitter <- length(eng_twitter)

# Identify number of characters in each media
char_blog <- round(mean(nchar(eng_blog)),2)
char_news <- round(mean(nchar(eng_news)),2)
char_twitter <- round(mean(nchar(eng_twitter)),2)

# Identify number of words in each media
word_blog <- wordcount(eng_blog, sep = " ")
word_news <- wordcount(eng_news, sep = " ")
word_twitter <- wordcount(eng_twitter, sep = " ")

# Identify word per line (WPL) in each media
wpl_blog <- round(word_blog/lines_blog,2)
wpl_news <- round(word_news/lines_news,2)
wpl_twitter <- round(word_twitter/lines_twitter,2)

# Summarize the data frame
file_names <- c("EN Blogs", "EN News", "EN Twitter")
file_sizes <- c(size_blog, size_news, size_twitter)
file_char <- c(char_blog, char_news,char_twitter)
file_lines <- c(lines_blog, lines_news, lines_twitter)
file_words <- c(word_blog, word_news, word_twitter)
avg_wpl <- c(wpl_blog, wpl_news, wpl_twitter)

file_summary <- data.frame(file_names, file_sizes, file_lines, file_char, file_words, avg_wpl)

colnames(file_summary) <- c("Media", "Size (MB)", "# Lines", "Avg Chars", "# Words", "Avg WPL")

# Make it as a kable
kable(file_summary) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Sample Fraction for Media source

After summarizing the file's raw content, sampling fraction of the word gram shall be considered to analyze the words.

```{R Word Sampling, warning = FALSE}
# Setup sampling rate & random seeding
sample_rate = 0.05
set.seed(369)

# Sample the number based on the total fraction of media
ss_blog <- lines_blog * sample_rate
ss_news <- lines_news * sample_rate
ss_twitter <- lines_twitter * sample_rate

# Generate sample
sample_blog <- sample(eng_blog, ss_blog, replace = FALSE)
sample_news <- sample(eng_news, ss_news, replace = FALSE)
sample_twitter <- sample(eng_twitter, ss_twitter, replace = FALSE)

# Remove non-English characters from sampled data
sample_blog <- iconv(sample_blog, "latin1","ASCII", sub = "")
sample_news <- iconv(sample_news, "latin1","ASCII", sub = "")
sample_twitter <- iconv(sample_twitter, "latin1","ASCII", sub = "")

# Remove outliers to ensure only the interquatile range texts are in 
# (excluding extremely long/short articles)

IQR_select <- function(text_file){
  first <- quantile(nchar(text_file),0.25)
  third <- quantile(nchar(text_file),0.75)
  text_file <- text_file[nchar(text_file) > first]
  text_file <- text_file[nchar(text_file) < third]
}

# Use pre-written interquartile function to remove outliers.
sample_blog <- IQR_select(sample_blog)
sample_news <- IQR_select(sample_news)
sample_twitter <- IQR_select(sample_twitter)

# Combine all data set as one
sample_sum <- c(sample_blog, sample_news, sample_twitter)


# Sampled Text Saving and examine the dimension
writeLines(sample_sum, "rd/en_US.sample.txt")
saveRDS(sample_sum,"rd/en_US.sample.rds")

# Lines from sample_sum
ss_lines <- length(sample_sum)

# number of words from sample_sum
ss_words <- sum(stri_count_words(sample_sum))

# Remove pre-defined values to optimize memories
rm(eng_blog, eng_news, eng_twitter, sample_blog, sample_news, sample_twitter)
rm(word_blog, word_news, word_twitter, wpl_blog, wpl_news, wpl_twitter)
rm(lines_blog, lines_news, lines_twitter, mega, size_blog, size_news, size_twitter)
rm(fileURL, file_char, file_lines, file_names, file_sizes, file_words)
rm(char_blog, char_news, char_twitter, avg_apl)
```

But before the n-gram calculation, samples shall be cleaned without ambiguous information. The following code will execute it.

```{R Corpus Demonstration, warning = FALSE}
# Generate text corpus and show example
text_corpus <- Corpus(VectorSource(sample_sum)) 

print(as.character(text_corpus[[1]]))
```

The next step is to use the sampled data to filter and tidy up the text file (a.k.a corpus) for sampling. In this case, the customized function named `corpus_clean` will be generated to execute the following command:

  1. Remove, URL, Twitter handles and email patterns while convert to spaces by custom content transformer
  2. Convert all words to lowercases
  3. Remove common English stop words.
  4. Remove punctuation marks
  5. Remove numeric values
  6. Trim whitespace.
  7. Remove profanity
  8. Convert to a plain text document
  
```{R Corpus Setup, warning = FALSE}
# Read profanity language for input
profanity <- read.csv("rd/en_profanity.csv"
                      , header = FALSE, sep = "\n") %>%
             iconv("latin1", "ASCII", sub = "") 

# Remove twitter handle
remove_thdl <- function(x){gsub("@[^\\s]+","", x)}

# Remove hashtag
remove_hash <- function(x){gsub("#[^\\s]+","", x)}

# Remove URL
remove_url <- function(x){gsub("http[^[:space:]]*", "", x)}

# Remove Punctuation
remove_reg <-function(x) {gsub("[^[:alpha:][:space:]]*", "", x)}

# Write a clean Corpus Function
clean_transform <- function(docu_df){
  docu_df <- VCorpus(VectorSource(docu_df)) %>%
    tm_map(content_transformer(tolower)) %>%# to lower case
    tm_map(content_transformer(removePunctuation)) %>%
    tm_map(removeNumbers) %>%
    tm_map(removeWords, stopwords("english")) %>%
    tm_map(removeWords, profanity) %>%
    tm_map(content_transformer(remove_url)) %>%
    tm_map(content_transformer(remove_reg)) %>%
    tm_map(content_transformer(remove_hash)) %>%
    tm_map(content_transformer(remove_thdl)) %>%
    tm_map(stripWhitespace) %>%
    tm_map(PlainTextDocument)
  
    return(docu_df)
}

corpus_clean <- function (source_file) {
  docu <- VCorpus(VectorSource(source_file)) %>%
    
# Remove URL, Twitter Handles and email pattern
    tm_map(space_conv, "(f|ht)tp(s?)://(.*)[.][a-z]+") %>%
    tm_map(space_conv, "@[^\\s]+") %>%
    tm_map(space_conv, "#[^\\s]+") %>%
    tm_map(space_conv, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b") %>%
    tm_map(space_conv, "[^[:alpha:][:space:]]*") %>%
    tm_map(removeWords, profanity) %>%
    tm_map(tolower) %>%
    tm_map(removeWords, stopwords("english")) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers) %>%
    tm_map(stripWhitespace) %>%
    tm_map(PlainTextDocument)
  
  return(docu)
}

clean_corpus <- clean_transform(sample_sum)

# Save clean corpus
saveRDS(clean_corpus, file = "rd/en_US.corpus.rds")

# Convert corpus to a data frame
clean_text <- data.frame(text = unlist(sapply(clean_corpus, '[', "content"))
                         , stringsAsFactors = FALSE)

writeLines(clean_text$text, "rd/en_US.corpus.txt")

# Remove unused terms to spare up memories
rm(text_corpus, sample_sum, corpus_clean, space_conv,IQR_select, ss_blog, ss_news
   ,ss_twitter, ss_words, ss_lines, avg_wpl, clean_transform, remove_hash,
   remove_reg, remove_thdl, remove_url)
```

## Initial Exploratory Data Analysis

Before executing the exploratory analysis with the cleaned corpus, document term matrix shall be generated while remove necessary sparse terms.

```{R DTM Generation, warning = FALSE}
# generate the Document Term Matrix while eliminating sparse terms

tdm_corpus <- TermDocumentMatrix(clean_corpus) %>%
  removeSparseTerms(sparse = 0.995)

# Convert the word counts before summarizing into the data frame
freq <- sort(rowSums(as.matrix(tdm_corpus)), decreasing = TRUE)

wordCount <- data.frame(word = names(freq), count = freq)
```

```{R Word Graph Setup, warning = FALSE}
## Top 12 unigram across the media
top12 <- wordCount[1:12,] %>%
  ggplot(aes(x = count,y = reorder(word, count))) +
  geom_bar(stat = "identity", fill = "#25BA95", alpha = 0.8) +
  labs(x = "Frequency", y = "Unigram", title = "Top 12 unigram across the media") +
  theme(axis.text.x = element_text(size = 7),
                       axis.text.y = element_text(size = 7),
                       axis.title = element_text(size = 8.5),
                       title = element_text(size = 9)) +
  geom_text(aes(label = wordCount[1:12,]$count), 
                           size = 2, hjust = -0.2) +
  xlim(0,9000)

# Display actual graph
top12
```

```{R Wordcloud Setup, warning = FALSE}
## Generate Wordcloud
png("jhu_ds_milestone.png", width=12 , height=8, units='in', res=300)
wordcloud(words = wordCount$word, freq = wordCount$count, min.freq = 150, scale = c(7, .25),max.words = 150, rot.per = 0.3, colors = brewer.pal(9, "Set2"))

## Remove unnecessary terms to free up memory
rm(tdm_corpus,freq, top12, wordCount)
```


## n-gram Tokenization & Distribution
After understanding the prelimnary distribution from the cleaned corpus, it's time to setup the ngram to distinguish the existing data.

But before the split for ngram, the preliminary tokenization shall be initiated to split the ngrams

```{r Tokenize Setup, warning = FALSE, message = FALSE}
# Unigram
token_n1 <- function(x){
  NGramTokenizer(x, Weka_control(min = 1, max = 1))
}  
  
# Bigram
token_n2 <- function(x){
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}  

# Trigram
token_n3 <- function(x){
  NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
```

```{R ngram Matrix Setup, message = FALSE, warning = FALSE}
# Generate respective matrices for the ngrams
matrix_n1 <- TermDocumentMatrix(clean_corpus, control = list(tokenize = token_n1))

matrix_n2 <- TermDocumentMatrix(clean_corpus, control = list(tokenize = token_n2))

matrix_n3 <- TermDocumentMatrix(clean_corpus, control = list(tokenize = token_n3))

# Eliminate sparse terms for each n gram and obtain frequency for n grams.
matrix_n1f <- sort(rowSums(as.matrix(removeSparseTerms(matrix_n1, 0.999))), decreasing = TRUE)

matrix_n2f <- sort(rowSums(as.matrix(removeSparseTerms(matrix_n2, 0.999))), decreasing = TRUE)

matrix_n3f <- sort(rowSums(as.matrix(removeSparseTerms(matrix_n3, 0.9999))), decreasing = TRUE)

# Generate data frame for the plotting in n gram
matrix_n1df <- data.frame(word = names(matrix_n1f), count = matrix_n1f)
matrix_n2df <- data.frame(word = names(matrix_n2f), count = matrix_n2f)
matrix_n3df <- data.frame(word = names(matrix_n3f), count = matrix_n3f)
```

```{R ngram Graphing, message = FALSE, warning = FALSE}
# Top 12 unigram across the media
top12_n1 <- matrix_n1df[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#0394fc", alpha = 0.8) +
  labs(x = "Unigram", y = "Frequency", title = "Top 12 unigram across the media") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 8.5),
        title = element_text(size = 9)) +
  geom_text(aes(label = matrix_n1df[1:12,]$count), size = 3, vjust = -0.2) +
  ylim(0,3800)

# Top 12 bigram across the media
top12_n2 <- matrix_n2df[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#5e0599", alpha = 0.8) +
  labs(x = "Bigram", y = "Frequency", title = "Top 12 biigram across the media") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 8.5),
        title = element_text(size = 9)) +
  geom_text(aes(label = matrix_n2df[1:12,]$count), size = 3, vjust = -0.2) +
  ylim(0,310)

# Top 12 trigram across the media
top12_n3 <- matrix_n3df[1:12,] %>%
  ggplot(aes(x = reorder(word, -count), y = count)) +
  geom_bar(stat = "identity", fill = "#649e97", alpha = 0.8) +
  labs(x = "Trigram", y = "Frequency", title = "Top 12 trigram across the media") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 8.5),
        title = element_text(size = 9)) +
  geom_text(aes(label = matrix_n3df[1:12,]$count), size = 3, vjust = -0.2) +
  ylim(0,35)

# Display Graphs
top12_n1
top12_n2
top12_n3
```

Based on the studying of the graph, the top 5 terms for respective media usage are summarized as follows:
```{R ngram summary, message = FALSE, warning = FALSE}
ngrams <- c("unigram", "bigram","trigram")
no1word <- c(matrix_n1df[1,1],matrix_n2df[1,1],matrix_n3df[1,1])
no2word <- c(matrix_n1df[2,1],matrix_n2df[2,1],matrix_n3df[2,1])
no3word <- c(matrix_n1df[3,1],matrix_n2df[3,1],matrix_n3df[3,1])
no4word <- c(matrix_n1df[4,1],matrix_n2df[4,1],matrix_n3df[4,1])
no5word <- c(matrix_n1df[5,1],matrix_n2df[5,1],matrix_n3df[5,1])

ngram_summary <- data.frame(ngrams, no1word, no2word, no3word, no4word, no5word)

colnames(ngram_summary) <- c("ngrams", "Top 1", "Top 2", "Top 3", "Top 4", "Top 5")

kable(ngram_summary) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Path Forward

After summarizing the existing data from SwiftKey and understand the current distribution between the words. The upcoming challenge is to establish a predictive model which will be deployed as a Shiny App for predicting in textbox.

The predictive algorithm will be developed via ngram models with word frequency similar towards the milestone report.

The strategy can be breaking down to the following 

  1. Find ways to neutralize the processing time for data sets.
  2. Use the text mining algorithm to obtain necessary ngrams.
  3. Review each ngram's methodology for performance.
  4. Decide necessary algorithm or better dictionary for better n-gram splits.
  5. Have different sets of non-common and common words for evaluation.
