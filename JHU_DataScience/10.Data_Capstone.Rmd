---
title: "Data Science Capstone Milestone Report"
author: "Hsin Chih Chen"
date: "11/1/2021"
output: html_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

This milestone reports covers the initial exploratory data analysis while summarizing necessary information from the swift cloud data.

## Pre-Loading Setup for Packages
```{r Package Loading, echo = FALSE}
library(tidytext)
library(tidyverse)
library(stringr)
library(knitr)
library(wordcloud)
library(ngram)
library(ggplot2)
library(dplyr)
library(tm)
library(data.table)
```

## Raw Data File Setup & Diagnosis
```{r Data Obtainment, echo = FALSE}
## Generate Directory
if(!file.exists("capstone")){
  dir.create("capstone")
}

## Download Raw Data
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(url, dest = "swiftkey.zip", mode = "wb")

## Unzip to Destination
unzip("swiftkey.zip", exdir = "./capstone")

## Import the Data files' directory
file_blogs <- "./capstone/final/en_US/en_US.blogs.txt"
file_news <- "./capstone/final/en_US/en_US.news.txt"
file_twitter <- "./capstone/final/en_US/en_US.twitter.txt"
```

```{r Data Diagnosis, echo = FALSE}
## Verify File Size (divide by MB units which is 2^20)
mega <- 2^20
size_blogs <- round(file.size(file_blogs)/ mega, 2)
size_news <- round(file.size(file_news)/ mega, 2)
size_twitter <- round(file.size(file_twitter)/ mega, 2)

## Extract information from the sources
blogs <- read_lines(file_blogs)
news <- read_lines(file_news)
twitter <- read_lines(file_twitter)

## Extract Number of Lines per File
lines_blogs <- length(blogs)
lines_news <- length(news)
lines_twitter <- length(twitter)
lines_total <- lines_blogs + lines_news + lines_twitter

## Extract Number of Characters per Line
chars_blogs <- nchar(blogs)
chars_news <- nchar(news)
chars_twitter <- nchar(twitter)


## Summarize Character Distribution in Boxplot
boxplot(chars_blogs, chars_news, chars_twitter, log = "y", names = c("blogs","news","twitter"),ylab = "log(Number of Characters)", xlab = "File Source")
title("Compare Characters per Line's Distritbuion")
```

```{r Data Summaries, echo = FALSE}
## Summarize the characters in each file
sumchar_blogs <- sum(chars_blogs)
sumchar_news <- sum(chars_news)
sumchar_twitter <- sum(chars_twitter)

## Summarize the Words in Each File
words_blogs <- wordcount(blogs, sep = " ")
words_news <- wordcount(news, sep = " ")
words_twitter <- wordcount(twitter, sep = " ")

## Summarize in Data Table

abstract <- data.frame(f_name = c("blogs", "news", "twitter"),
                       size = c(size_blogs, size_news, size_twitter), 
                       lines = c(lines_blogs, lines_news, lines_twitter),
                       n_chars = c(sumchar_blogs, sumchar_news, sumchar_twitter),
                       n_words = c(words_blogs, words_news, words_twitter))

abstract <- abstract %>% mutate(pct_char = round(n_chars/sum(n_chars),2))
abstract <- abstract %>% mutate(pct_lines = round(lines/sum(lines),2))
abstract <- abstract %>% mutate(pct_words = round(n_words/sum(n_words),2))

kable(abstract)
```

## Perform Sample Fraction for Words

```{r Sample Obtainment}

## Setup the sample size in terms of lines before sampling

sample_rate = 0.05
set.seed(369)
ss_blogs <- lines_blogs * sample_rate
ss_news <- lines_news * sample_rate
ss_twitter <- lines_twitter * sample_rate

## Generate samples
sample_blogs <- sample(blogs, ss_blogs)
sample_news <- sample(news, ss_news)
sample_twitter <- sample(twitter, ss_twitter)
sample_abstract <- c(sample_blogs, sample_news, sample_twitter)

## Save the sampled
writeLines(sample_abstract, "./capstone/final/en_US/en_US.sample_abstract.txt")
saveRDS(sample_abstract, "./capstone/final/en_US/sample_abstract.rds")
```

## Sample Cleaning
```{r Sample Cleaning, echo = FALSE}
## Clean the sample abstract and illustrate examples
cleaned <- Corpus(VectorSource(sample_abstract))
print(as.character(cleaned[[1]]))

## Filtering URL, Punctuation & Numbers with Functions
URLRemoval <- function(x){
  gsub("http[^[:space:]]*", "", x)
}
NumPunctRemoval <- function(x){
  gsub("[^[:alpha:][:space:]]*", "", x)
}

## Execute Cleaning for URL, Punctuation & Numbers with Functions
cleaned <- tm_map(cleaned, content_transformer(URLRemoval))
cleaned <- tm_map(cleaned, content_transformer(NumPunctRemoval))

## Translate all sample to lower cases
cleaned <- tm_map(cleaned, content_transformer(tolower))

## Create Profanity Filter Source
profanity <- read.csv("./capstone/final/en_us/en_profanity.csv", header = FALSE, sep = "\n")

## Remove Profanity
cleaned <- tm_map(cleaned, removeWords, profanity[,1])

## Remove Stopwords
cleaned <- tm_map(cleaned, removeWords, stopwords("english"))
cleaned <- tm_map(cleaned, removeWords, stopwords("SMART"))

## Quick demonstrate for dropped terms
print(as.character(cleaned[[1]]))

## Remove whitespaces
cleaned <- tm_map(cleaned, stripWhitespace)
print(as.character(cleaned[[1]]))

## Save Clean Corpus
saveRDS(cleaned, file = "./capstone/final/en_US/cleaned.rds")
```

## Initial Exploratory Data Analysis
```{R Initial Data Analysis, echo = FALSE}
## Generate the document term matrix with cleaned sample
doc_corpus <- DocumentTermMatrix(cleaned)
dim(doc_corpus)

## Remove sparse terms 
sdoc_corpus <- removeSparseTerms(doc_corpus, sparse = 0.995)
dim(sdoc_corpus)

## Establish Frequent Terms Calculation
wordcount <- colSums(as.matrix(sdoc_corpus))
length(wordcount)

word_feat <- data.table(name = attributes(wordcount)$names, count = wordcount)

## Check Most & Least Frequent Vocabs

word_feat[order(-count)][1:10] ## Most Frequent

word_feat[order(count)][1:10] ## Least Frequent

#
```
